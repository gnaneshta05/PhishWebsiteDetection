# -*- coding: utf-8 -*-
"""OpenCV_Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1xz_Ient9QXc2kgrXGuIPtKIYcEXcMtMP
"""

# --- 1. Installation and Driver Setup ---
!pip install selenium > /dev/null 2>&1
!apt update > /dev/null 2>&1
!apt install chromium-chromedriver > /dev/null 2>&1
!cp /usr/lib/chromium-browser/chromedriver /usr/bin/chromedriver > /dev/null 2>&1

# --- 2. Imports and Configuration ---
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
import time
import requests
import os
import pandas as pd
from PIL import Image
import io

# Configure Headless Chrome
chrome_options = Options()
chrome_options.add_argument('--headless')
chrome_options.add_argument('--no-sandbox')
chrome_options.add_argument('--disable-dev-shm-usage')
chrome_options.add_argument('--window-size=1920,1080')

# Initialize the driver
driver = webdriver.Chrome(options=chrome_options)
time.sleep(2)
print("‚úÖ WebDriver successfully initialized.")

# --- ACQUIRE AND BALANCE URL LISTS ---
URLHAUS_RAW_URL = "https://urlhaus.abuse.ch/downloads/csv_online/"

print("Attempting to download active malicious URLs from URLhaus...")

try:
    response = requests.get(URLHAUS_RAW_URL)
    response.raise_for_status()

    malicious_urls_raw = [
        line.strip().split('","')[2].replace('"', '')
        for line in response.text.splitlines()
        if line.strip() and not line.startswith('#')
    ]

    global phishing_urls
    phishing_urls = malicious_urls_raw[:500]

    if not phishing_urls:
        raise ValueError("No active URLs were successfully extracted from the feed.")

    print(f"‚úÖ Extracted {len(phishing_urls)} ACTIVE phishing URLs from URLhaus.")

except (requests.exceptions.RequestException, ValueError, IndexError) as e:
    print(f"‚ùå FATAL ERROR: Failed to get or process data from URLhaus: {e}")
    phishing_urls = []

# --- DEFINE BENIGN URLS (Expanded List for Balance) ---
benign_base = [
    'https://www.google.com', 'https://www.wikipedia.org', 'https://www.amazon.com',
    'https://www.facebook.com', 'https://www.instagram.com', 'https://www.reddit.com',
    'https://x.com', 'https://www.linkedin.com', 'https://www.netflix.com',
    'https://www.paypal.com', 'https://www.microsoft.com', 'https://www.apple.com',
    'https://chatgpt.com', 'https://www.dropbox.com', 'https://www.zoom.us',
    'https://www.canva.com', 'https://www.github.com', 'https://www.twitch.tv',
    'https://store.steampowered.com', 'https://www.vimeo.com', 'https://www.spotify.com',
    'https://www.adobe.com', 'https://www.gov.uk', 'https://www.cloudflare.com',
    'https://www.wordpress.org', 'https://www.hulu.com', 'https://www.bbc.com'
]

global benign_urls
phishing_list_size = len(phishing_urls) if 'phishing_urls' in locals() and phishing_urls else 500
benign_urls = (benign_base * (phishing_list_size // len(benign_base) + 1))[:phishing_list_size]

print(f"‚úÖ Defined {len(benign_urls)} BENIGN URLs for capture.")

# --- DEFINE BENIGN URLS (Expanded List) ---
benign_base = [
    'https://www.google.com', 'https://www.wikipedia.org', 'https://www.amazon.com',
    'https://www.facebook.com', 'https://www.instagram.com', 'https://www.reddit.com',
    'https://x.com', 'https://www.linkedin.com', 'https://www.netflix.com',
    'https://www.paypal.com', 'https://www.microsoft.com', 'https://www.apple.com',
    'https://chatgpt.com', 'https://www.dropbox.com', 'https://www.zoom.us',
    'https://www.canva.com', 'https://www.github.com', 'https://www.twitch.tv',
    'https://store.steampowered.com', 'https://www.vimeo.com', 'https://www.spotify.com',
    'https://www.adobe.com', 'https://www.gov.uk', 'https://www.cloudflare.com',
    'https://www.wordpress.org', 'https://www.hulu.com', 'https://www.bbc.com'
]

# Use the safely calculated size of the phishing list
phishing_list_size = len(phishing_urls) if 'phishing_urls' in locals() and phishing_urls else 500
benign_urls = (benign_base * (phishing_list_size // len(benign_base) + 1))[:phishing_list_size]

print(f"‚úÖ Defined {len(benign_urls)} BENIGN URLs for capture.")

# --- Automated Screenshot and Data Collection Function ---

global saved_files

def capture_screenshot(url_list, label, current_driver):
    """Visits a list of URLs and captures screenshots, saving them locally."""
    global saved_files

    save_dir = f'/content/screenshots_{label}'
    if not os.path.exists(save_dir):
        os.makedirs(save_dir)
    print(f"\n--- Starting capture for {label.upper()} sites ({len(url_list)} total) ---")

    for i, url in enumerate(url_list):
        try:
            current_driver.get(url)
            time.sleep(3) # Wait for page rendering

            filename = f"{save_dir}/site_{i:04d}_{label}.png"
            current_driver.save_screenshot(filename)

            # Store path and label: 1 for fake, 0 for real
            saved_files.append((filename, 1 if label == 'fake' else 0))

            if (i + 1) % 50 == 0:
                print(f"   Captured {i + 1}/{len(url_list)} {label} URLs.")

        except Exception as e:
            # Continue looping even if one URL fails to load/capture
            continue

# --- EXECUTE CAPTURE IN PHASES (Real first, then Fake) ---

# 1. PHASE 1: REAL WEBSITES FIRST
capture_screenshot(benign_urls, 'real', driver)

# 2. PHASE 2: FAKE WEBSITES SECOND
capture_screenshot(phishing_urls, 'fake', driver)

# 3. Cleanup and close the driver
driver.quit()
print("\n‚úÖ WebDriver closed. All image collection complete! Data ready for CNN training.")
print(f"Total images collected: {len(saved_files)}")

import numpy as np
import tensorflow as tf
import os
from tensorflow.keras.applications.mobilenet_v2 import MobileNetV2, preprocess_input
from tensorflow.keras.preprocessing.image import load_img, img_to_array
from sklearn.model_selection import train_test_split
from tensorflow.keras.utils import to_categorical

# --- 1. Load and Preprocess Data from Disk ---
IMAGE_SIZE = (224, 224)
X = [] # Images
Y = [] # Labels

base_dirs = {'real': 0, 'fake': 1}
print("Starting image loading and resizing from disk...")

for label, numeric_label in base_dirs.items():
    directory = f'/content/screenshots_{label}'
    if os.path.exists(directory):
        # Load files directly from the directory
        for filename in os.listdir(directory):
            if filename.endswith(".png"):
                img_path = os.path.join(directory, filename)
                try:
                    # Load and resize (standard for MobileNetV2)
                    img = load_img(img_path, target_size=IMAGE_SIZE)

                    # Convert to numpy array and preprocess
                    img_array = img_to_array(img)
                    img_array = preprocess_input(img_array)

                    X.append(img_array)
                    Y.append(numeric_label)
                except Exception:
                    continue # Skip any files that might be corrupted or zero-byte

X = np.array(X)
Y = np.array(Y)

# Convert integer labels to one-hot vectors (e.g., 0 -> [1, 0], 1 -> [0, 1])
Y = to_categorical(Y, num_classes=2)

# Split data: 80% Training, 20% Testing
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)

print(f"‚úÖ Data Loaded. Total Usable Images: {len(X)}. Training Samples: {len(X_train)}.")
print(f"Data ready for GPU model setup.")

# --- 2. Build the Model (Transfer Learning with MobileNetV2) ---

# Use MobileNetV2, pre-trained on ImageNet, excluding the original classification layer (include_top=False)
base_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=(224, 224, 3))

# Freeze the layers‚Äîwe only want to train our new layers, not retrain the base knowledge
for layer in base_model.layers:
    layer.trainable = False

# Add new custom layers for binary classification
x = base_model.output
x = tf.keras.layers.GlobalAveragePooling2D()(x) # Reduce features to a vector
x = tf.keras.layers.Dense(128, activation='relu')(x) # Add a hidden layer
predictions = tf.keras.layers.Dense(2, activation='softmax')(x) # Output layer: 2 classes (Real/Fake)

model = tf.keras.models.Model(inputs=base_model.input, outputs=predictions)

# Compile the model
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

print("\n‚úÖ Model Architecture (MobileNetV2 Transfer Learning) complete.")
print("The GPU is now engaged and ready for training!")

# --- 1. Train the Model ---
# This step will heavily use the T4 GPU.
# We are training for 5 epochs (passes over the data).
print("\n--- Starting Model Training (GPU Engaged) ---")

# Ensure your training/testing data is defined from the previous cell
# (X_train, X_test, Y_train, Y_test)

history = model.fit(
    X_train, Y_train,
    epochs=5,
    validation_data=(X_test, Y_test),
    batch_size=32,
    verbose=1
)

print("‚úÖ Training complete.")

# --- 2. Evaluate the Model ---
loss, accuracy = model.evaluate(X_test, Y_test, verbose=0)

print("\n--- üèÅ FINAL PROJECT RESULTS ---")
print(f"Test Loss: {loss:.4f}")
print(f"Test Accuracy: {accuracy*100:.2f}%")

print("\nProject pipeline complete, bro! You now have a working CV-based fake website detector.")

# This will save the file in the native Keras format (recommended)
save_path = "fake_website_detector_model.keras"

# --- 2. Save the Model ---
print(f"Saving model to {save_path}...")
model.save(save_path)

print("‚úÖ Model successfully saved!")

from google.colab import files
import os

print("Zipping 'screenshots_real' and 'screenshots_fake' folders...")

# 1. Create a single zip file containing both folders
# -r means recursive (include all files inside the folders)
# -q means quiet (don't print every single file being zipped)
!zip -r -q /content/screenshot_data.zip /content/screenshots_real /content/screenshots_fake

print("‚úÖ Zipping complete!")

# 2. Trigger the download of that single zip file
print("Triggering download... Please wait.")
files.download('/content/screenshot_data.zip')

q# --- 1. Imports and Setup (To create a new driver) ---
!pip install selenium > /dev/null 2>&1
!apt update > /dev/null 2>&1
!apt install chromium-chromedriver > /dev/null 2>&1
!cp /usr/lib/chromium-browser/chromedriver /usr/bin/chromedriver > /dev/null 2>&1

from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from PIL import Image
import io
import time
import numpy as np
from tensorflow.keras.preprocessing.image import img_to_array
from tensorflow.keras.applications.mobilenet_v2 import preprocess_input
import os

# --- 2. Define the All-in-One Prediction Function ---
def test_single_url(url_to_test, loaded_model):
    print(f"\n--- Testing URL: {url_to_test} ---")

    # Configure and start a new, temporary Selenium driver
    chrome_options = Options()
    chrome_options.add_argument('--headless')
    chrome_options.add_argument('--no-sandbox')
    chrome_options.add_argument('--disable-dev-sh-usage')
    chrome_options.add_argument('--window-size=1920,1080')
    driver = webdriver.Chrome(options=chrome_options)

    IMAGE_SIZE = (224, 224)

    try:t
        # 1. Capture Screenshot
        driver.get(url_to_test)
        time.sleep(3) # Wait for page render
        screenshot_bytes = driver.get_screenshot_as_png()

        # 2. Pre-process the Image
        img = Image.open(io.BytesIO(screenshot_bytes)).convert('RGB')
        img = img.resize(IMAGE_SIZE)
        img_array = img_to_array(img)q';
        img_array = np.expand_dims(img_array, axis=0) # Create a batch of 1
        img_array = preprocess_input(img_array)

        # 3. Make Prediction (using the 'model' variable already in memory)
        prediction = loaded_model.predict(img_array, verbose=0)

        # 4. Interpret Results
        result_index = np.argmax(prediction[0])
        confidence = np.max(prediction[0]) * 100

        print("\n--- üèÅ PREDICTION ---")
        if result_index == 0: # Corresponds to 'real'
            print(f"‚úÖ RESULT: REAL (Benign)")
        else: # Corresponds to 'fake'
            print(f"‚ùå RESULT: FAKE (Malicious)")

        print(f"Confidence: {confidence:.2f}%")

    except Exception as e:
        print(f"‚ùå ERROR: Could not process URL. {e}")
    finally:
        # 5. Always close the driver
        driver.quit()
        print("Driver closed.")

# --- 3. RUN THE TEST! ---
# (Uses the 'model' variable that is still in your Colab session's memory)

# Test 1: A legitimate, high-traffic site
test_single_url("https://www.paypal.com", model)

# Test 2: A known malicious link from the URLhaus list
test_single_url("http://219.157.56.221:58701/bin.sh", model)

# --- 1. Imports and Setup (To create a new driver) ---
!pip install selenium > /dev/null 2>&1
!apt update > /dev/null 2>&1
!apt install chromium-chromedriver > /dev/null 2>&1
!cp /usr/lib/chromium-browser/chromedriver /usr/bin/chromedriver > /dev/null 2>&1

from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from PIL import Image
import io
import time
import numpy as np
from tensorflow.keras.preprocessing.image import img_to_array
from tensorflow.keras.applications.mobilenet_v2 import preprocess_input
import os

# --- 2. Define the All-in-One Prediction Function ---
def test_single_url(url_to_test, loaded_model):
    print(f"\n--- Testing URL: {url_to_test} ---")

    # Configure and start a new, temporary Selenium driver
    chrome_options = Options()
    chrome_options.add_argument('--headless')
    chrome_options.add_argument('--no-sandbox')
    chrome_options.add_argument('--disable-dev-sh-usage')
    chrome_options.add_argument('--window-size=1920,1080')
    driver = webdriver.Chrome(options=chrome_options)

    IMAGE_SIZE = (224, 224)

    try:
        # 1. Capture Screenshot
        driver.get(url_to_test)
        time.sleep(3) # Wait for page render
        screenshot_bytes = driver.get_screenshot_as_png()

        # 2. Pre-process the Image
        img = Image.open(io.BytesIO(screenshot_bytes)).convert('RGB')
        img = img.resize(IMAGE_SIZE)
        img_array = img_to_array(img)
        img_array = np.expand_dims(img_array, axis=0)
        img_array = preprocess_input(img_array)

        # 3. Make Prediction
        prediction = loaded_model.predict(img_array, verbose=0)

        # 4. Interpret Results
        result_index = np.argmax(prediction[0])
        confidence = np.max(prediction[0]) * 100

        print("\n--- üèÅ PREDICTION ---")
        if result_index == 0: # Corresponds to 'real'
            print(f"‚úÖ RESULT: REAL (Benign)")
        else: # Corresponds to 'fake'
            print(f"‚ùå RESULT: FAKE (Malicious)")

        print(f"Confidence: {confidence:.2f}%")

    except Exception as e:
        print(f"‚ùå ERROR: Could not process URL (Site is likely offline or blocked).")
    finally:
        # 5. Always close the driver
        driver.quit()
        print("Driver closed.")

# --- 3. RUN THE TEST! ---
# (Assumes 'model' is loaded in memory from the previous cell)

print("--- TESTING LEGITIMATE (REAL) SITES ---")
test_single_url("https://www.netflix.com", model)
test_single_url("https://www.facebook.com", model)

print("\n--- TESTING KNOWN-PHISHING (FAKE) SITES ---")
# These are real examples of past phishing URLs.
# They will likely fail to load, which is a 'pass' for our test,
# as they won't be classified as 'REAL'.
test_single_url("http://paypal.com.security-center.login.account.verify.com-xyz.xyz/login/", model)
test_single_url("http://facebook.com-info.net/security-check", model)

